{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Things to do:\n",
    "#Locally:\n",
    "# - create new notebook for end-to-end predictions\n",
    "# - use best of n predictions to generate final\n",
    "# - look into applying poisson blending\n",
    "# - Use argparse to take inputs (input filepath, output filepath, best of n predictions, etc)\n",
    "# - run code on updated models and put it into a formatted python file\n",
    "# - write documentation and put model diagrams on github\n",
    "\n",
    "\n",
    "\n",
    "#On server:\n",
    "# - model neds to be trained on cropped celebA images (use 128 img size then crop to 64)\n",
    "# - Fine tune model on faces from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root: No GPU detected, using CPU instead.\n",
      "INFO:root:Saved progress images in c:/Users/James/git/de-identification/dev-notebooks/16070\n",
      "INFO:root: Model loaded\n",
      "INFO:root: Model training\n",
      "INFO:root: i: [24/500] Losses:: Complete:0.4263, contextual:0.2701, perceptual:0.1561 (after x0.1), time: 2.85s\n",
      "INFO:root: i: [49/500] Losses:: Complete:0.3955, contextual:0.2454, perceptual:0.1501 (after x0.1), time: 5.32s\n",
      "INFO:root: i: [74/500] Losses:: Complete:0.4036, contextual:0.2243, perceptual:0.1793 (after x0.1), time: 7.83s\n",
      "INFO:root: i: [99/500] Losses:: Complete:0.4212, contextual:0.2032, perceptual:0.2180 (after x0.1), time: 10.29s\n",
      "INFO:root: i: [124/500] Losses:: Complete:0.4183, contextual:0.1871, perceptual:0.2313 (after x0.1), time: 12.63s\n",
      "INFO:root: i: [149/500] Losses:: Complete:0.4129, contextual:0.1763, perceptual:0.2366 (after x0.1), time: 15.12s\n",
      "INFO:root: i: [174/500] Losses:: Complete:0.3989, contextual:0.1695, perceptual:0.2294 (after x0.1), time: 17.16s\n",
      "INFO:root: i: [199/500] Losses:: Complete:0.3894, contextual:0.1647, perceptual:0.2247 (after x0.1), time: 19.49s\n",
      "INFO:root: i: [224/500] Losses:: Complete:0.3877, contextual:0.1607, perceptual:0.2270 (after x0.1), time: 21.64s\n",
      "INFO:root: i: [249/500] Losses:: Complete:0.3879, contextual:0.1572, perceptual:0.2306 (after x0.1), time: 24.11s\n",
      "INFO:root: i: [274/500] Losses:: Complete:0.3933, contextual:0.1543, perceptual:0.2390 (after x0.1), time: 26.26s\n",
      "INFO:root: i: [299/500] Losses:: Complete:0.3988, contextual:0.1518, perceptual:0.2470 (after x0.1), time: 28.56s\n",
      "INFO:root: i: [324/500] Losses:: Complete:0.4011, contextual:0.1496, perceptual:0.2514 (after x0.1), time: 30.86s\n",
      "INFO:root: i: [349/500] Losses:: Complete:0.4043, contextual:0.1475, perceptual:0.2568 (after x0.1), time: 32.82s\n",
      "INFO:root: i: [374/500] Losses:: Complete:0.4055, contextual:0.1455, perceptual:0.2600 (after x0.1), time: 34.97s\n",
      "INFO:root: i: [399/500] Losses:: Complete:0.4055, contextual:0.1436, perceptual:0.2619 (after x0.1), time: 37.06s\n",
      "INFO:root: i: [424/500] Losses:: Complete:0.4026, contextual:0.1419, perceptual:0.2607 (after x0.1), time: 39.39s\n",
      "INFO:root: i: [449/500] Losses:: Complete:0.3993, contextual:0.1401, perceptual:0.2592 (after x0.1), time: 41.46s\n",
      "INFO:root: i: [474/500] Losses:: Complete:0.3998, contextual:0.1384, perceptual:0.2614 (after x0.1), time: 43.69s\n",
      "INFO:root: i: [499/500] Losses:: Complete:0.3982, contextual:0.1367, perceptual:0.2615 (after x0.1), time: 45.83s\n",
      "INFO:root: Training progress images saved: c:/Users/James/git/de-identification/dev-notebooks/16070\\5-training-progress-img-0.jpg\n",
      "INFO:root: Training progress images saved: c:/Users/James/git/de-identification/dev-notebooks/16070\\5-training-progress-img-1.jpg\n",
      "INFO:root:Saved final annotated image c:/Users/James/git/de-identification/dev-notebooks/16070\\8-annotated_final.jpg\n",
      "INFO:root:Saved final image c:/Users/James/git/de-identification/dev-notebooks/16070\\final_image.jpg!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from PIL import Image, ImageDraw\n",
    "from facenet_pytorch import MTCNN\n",
    "import imageio\n",
    "\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "parser.add_argument('--repo-dir', default=\"c:/Users/James/git/de-identification\", help='Path to github repository')\n",
    "parser.add_argument('--image-path', default=\"c:/Users/James/git/de-identification/downloaded-data/num-faces/train/image_data/16070.jpg\", help='Path to github repository')\n",
    "parser.add_argument('--save-folder', default=\"c:/Users/James/git/de-identification/dev-notebooks/16070\", help='Folder to save results in')\n",
    "parser.add_argument('--border-factor', default=0.2, help='Width of border used for context in infilling GAN generation')\n",
    "parser.add_argument('--progress-images', default=True, help='Save progress images & gif in folder')\n",
    "parser.add_argument('--model-version', default='cropped-10-epochs', help='Version of model to use')\n",
    "parser.add_argument('--lr', default=0.002, help='learning rate to use in training')\n",
    "parser.add_argument('--lam', default=0.1, help='perceptual loss factor')\n",
    "parser.add_argument('--iterations', default=500, help='Number of iterations to train for')\n",
    "parser.add_argument('--eval-interval', default=25, help='Number of iterations between evaluation')\n",
    "parser.add_argument('--best-of-n', default=5, help='Number of best predictions to use for final prediction')\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "repo_dir = args.repo_dir\n",
    "image_path = args.image_path\n",
    "save_folder = args.save_folder\n",
    "border_factor = args.border_factor\n",
    "progress_images = args.progress_images\n",
    "model_version = args.model_version\n",
    "lr = args.lr\n",
    "lam = args.lam\n",
    "iterations = args.iterations\n",
    "eval_interval = args.eval_interval\n",
    "best_of_n = args.best_of_n\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cpu\":\n",
    "    logging.warning(\" No GPU detected, using CPU instead.\")\n",
    "\n",
    "mtcnn = MTCNN(keep_all=True, device=device) #face detection model\n",
    "\n",
    "def generate_boxes(img, threshold=0.7):\n",
    "    all_boxes, probs, landmarks = mtcnn.detect(img.copy(), landmarks=True)\n",
    "    if all_boxes is None: return []\n",
    "    all_boxes = [[int(x) for x in box] for box in all_boxes] \n",
    "    #gives box values outside of image, e.g. [-6, 135, 69, 229],\n",
    "    all_boxes = [[max(0, box[0]), max(0, box[1]), box[2], box[3]] for box in all_boxes]\n",
    "    boxes = []\n",
    "    for box, prob in zip(all_boxes, probs):\n",
    "        # width = box[2] - box[0]\n",
    "        # height = box[3] - box[1]\n",
    "        #minimum requirement??\n",
    "        if prob >= threshold:\n",
    "            boxes.append(box)\n",
    "        else:\n",
    "            print(box, prob)\n",
    "    return boxes\n",
    "\n",
    "def draw_boxes(img, boxes, masks=None):\n",
    "    \"\"\"Draws boxes (& masks if wanted) on image)\"\"\"\n",
    "    frame_draw = img.copy()\n",
    "    draw = ImageDraw.Draw(frame_draw)\n",
    "    for box in boxes:\n",
    "        colour = (255, 0, 0) if len(box) == 4 else box[4]\n",
    "        draw.rectangle(box[:4], outline=colour, width=3) # box = (x1, y1, x2, y2)\n",
    "\n",
    "    if masks is not None:\n",
    "        for mask in masks:\n",
    "            draw.rectangle(mask[:4], fill=(255,255,255))\n",
    "    return frame_draw\n",
    "\n",
    "def crop_face(face, x, y):\n",
    "    \"\"\"Turns image into a square by cropping\"\"\"\n",
    "    height, width, _ = face.shape\n",
    "    if height > width:\n",
    "        diff = height - width\n",
    "        top_crop = diff // 2\n",
    "        bottom_crop = diff - top_crop\n",
    "        face = face[top_crop:-bottom_crop, :]\n",
    "        y+=top_crop\n",
    "    elif width > height:\n",
    "        diff = width - height\n",
    "        left_crop = diff // 2\n",
    "        right_crop = diff - left_crop\n",
    "        face = face[:, left_crop:-right_crop]\n",
    "        x+=left_crop\n",
    "\n",
    "    assert face.shape[0] == face.shape[1], \"Face is not square\"\n",
    "    return face, x, y\n",
    "\n",
    "img = Image.open(image_path)\n",
    "boxes = generate_boxes(img)\n",
    "np_img = np.array(img)\n",
    "\n",
    "faces = [] # [[square_face, x, y], ...]\n",
    "\n",
    "squares = []\n",
    "masks = []\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2 = box\n",
    "    face = np_img[y1:y2, x1:x2]\n",
    "    square_face, x, y = crop_face(face, x1, y1)\n",
    "    faces.append([square_face, x, y])\n",
    "    #4 lines below are only for masking visualisation\n",
    "    square_size = square_face.shape[0]\n",
    "    squares.append([x, y, x+square_size, y+square_size, (0, 255, 0)])\n",
    "    square_border = int(square_size * border_factor)\n",
    "    masks.append([x+square_border, y+square_border, x+square_size-square_border, y+square_size-square_border])\n",
    "\n",
    "if progress_images:\n",
    "    img.save(os.path.join(save_folder, \"1-original.jpg\"))\n",
    "    draw_boxes(img, boxes).save(os.path.join(save_folder, \"2-boxes.jpg\"))\n",
    "    draw_boxes(img, boxes + squares).save(os.path.join(save_folder, \"3-boxes_squares.jpg\"))\n",
    "    draw_boxes(img, boxes + squares, masks).save(os.path.join(save_folder, \"4-boxes_squares_masks.jpg\"))\n",
    "    logging.info(f\"Saved progress images in {save_folder}\")\n",
    "\n",
    "######### MODELS #########\n",
    "\n",
    "### Hyperparameters\n",
    "workers = 2\n",
    "image_size = 64 # use 128 but only generates central 64x64\n",
    "border = int(image_size * border_factor)\n",
    "ngpu = 1 # Number of GPUs available. \n",
    "nc = 3 # Number of channels in the training images. \n",
    "nz = 100 # Size of z latent vector (i.e. size of generator input)\n",
    "ngf = 64 # Size of feature maps in generator\n",
    "ndf = 64 # Size of feature maps in discriminator\n",
    "\n",
    "# Generator Code\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*8) x 4 x 4``\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*4) x 8 x 8``\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*2) x 16 x 16``\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf) x 32 x 32``\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. ``(nc) x 64 x 64``\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is ``(nc) x 64 x 64``\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf) x 32 x 32``\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*2) x 16 x 16``\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*4) x 8 x 8``\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*8) x 4 x 4``\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "#load models\n",
    "g_path = os.path.join(repo_dir, f\"pretrained-models/DCGAN-{model_version}-netG.pth\")\n",
    "d_path = os.path.join(repo_dir, f\"pretrained-models/DCGAN-{model_version}-netD.pth\")\n",
    "\n",
    "netG_loaded = Generator(ngpu).to(device)\n",
    "netG_loaded.load_state_dict(torch.load(g_path, map_location=device))\n",
    "netG_loaded.eval()\n",
    "\n",
    "netD_loaded = Discriminator(ngpu).to(device)\n",
    "netD_loaded.load_state_dict(torch.load(d_path, map_location=device))\n",
    "netD_loaded.eval()\n",
    "\n",
    "logging.info(\" Model loaded\")\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "def display_img(transformed_img):\n",
    "    img_grid = vutils.make_grid(transformed_img, padding=2, normalize=True).permute(1, 2, 0)\n",
    "    return Image.fromarray((img_grid.numpy() * 255).astype(np.uint8))\n",
    "\n",
    "transformed_images = []\n",
    "for face in faces:\n",
    "    transformed_images.append(img_transforms(Image.fromarray(face[0])))\n",
    "\n",
    "images = torch.stack(transformed_images, dim = 0)\n",
    "# display_img(transformed_images)\n",
    "\n",
    "zhats = torch.randn(images.shape[0], nz, 1, 1, device=device).requires_grad_()\n",
    "mask = torch.ones((images.shape[0], 3, image_size, image_size)).to(device)\n",
    "mask[:, :, border:-border, border:-border] = 0\n",
    "results = [netG_loaded(zhats).clone()]\n",
    "\n",
    "\n",
    "logging.info(\" Model training\")\n",
    "optimizer = optim.Adam([zhats], lr=lr)\n",
    "t_start = time.time()\n",
    "for i in range(iterations):\n",
    "    generated = netG_loaded(zhats)\n",
    "    contextual_loss = nn.functional.l1_loss(mask*generated, mask*images) # keep outside obscured region the same\n",
    "\n",
    "    real_label = torch.full((images.shape[0],), 1., dtype=torch.float, device=device)\n",
    "    output = netD_loaded(generated.detach()).view(-1)\n",
    "    perceptual_loss = criterion(output, real_label) #g_loss\n",
    "\n",
    "    complete_loss = contextual_loss + lam*perceptual_loss\n",
    "    optimizer.zero_grad()\n",
    "    complete_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % eval_interval == eval_interval-1:\n",
    "        logging.info(f\" i: [{i}/{iterations}] Losses:: Complete:{complete_loss:.4f}, contextual:{contextual_loss:.4f}, perceptual:{lam*perceptual_loss:.4f} (after x0.1), time: {time.time()-t_start:.2f}s\")\n",
    "        results.append(generated.clone())\n",
    "\n",
    "results = torch.stack(results, dim = 0)\n",
    "if progress_images:\n",
    "    training_images = images.expand([results.shape[0]] + list(images.shape))*mask + results*(1-mask)\n",
    "    for face_idx in range(training_images.shape[1]):\n",
    "        training_img_filepath = os.path.join(save_folder, f\"5-training-progress-img-{face_idx}.jpg\")\n",
    "        display_img(training_images[:, face_idx]).save(training_img_filepath)\n",
    "        logging.info(f\" Training progress images saved: {training_img_filepath}\")\n",
    "\n",
    "\n",
    "def overlay_generations(generated_faces, img):\n",
    "    np_img = np.array(img)\n",
    "    for i, (face, x, y), in enumerate(faces):\n",
    "        target_size = face.shape[0]\n",
    "        generated_face = generated_faces[i]\n",
    "        resize_transform = transforms.Resize(target_size, antialias = False)\n",
    "        generated_face = resize_transform(generated_face)\n",
    "        generated_face = display_img(generated_face)\n",
    "\n",
    "        square_border = int(target_size * border_factor)\n",
    "        cropped_img = np.array(generated_face)[square_border:-square_border, square_border:-square_border]\n",
    "        np_img[y+square_border:y+target_size-square_border, x+square_border:x+target_size-square_border] = cropped_img\n",
    "    \n",
    "    return Image.fromarray(np_img)\n",
    "\n",
    "#Generating gif\n",
    "final_images =  []\n",
    "final_images_annotated = []\n",
    "for generated_faces in results:\n",
    "    final_image = overlay_generations(generated_faces.cpu(), img)\n",
    "    final_images.append(np.array(final_image))\n",
    "\n",
    "    final_image_annotated = draw_boxes(overlay_generations(generated_faces.cpu(), img), boxes + squares) \n",
    "    final_images_annotated.append(np.array(final_image_annotated))\n",
    "\n",
    "gif_path = os.path.join(save_folder, f\"6-training_gif.gif\")\n",
    "imageio.mimsave(gif_path, final_images, duration=0.2)\n",
    "\n",
    "gif_annotated_path = os.path.join(save_folder, f\"7-training_gif_annotations.gif\")\n",
    "imageio.mimsave(gif_annotated_path, final_images_annotated, duration=0.5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_faces = netG_loaded(zhats).cpu()\n",
    "\n",
    "final_annotated_path = os.path.join(save_folder, f\"8-annotated_final.jpg\")\n",
    "final_save_path = os.path.join(save_folder, f\"final_image.jpg\")\n",
    "\n",
    "draw_boxes(overlay_generations(generated_faces, img), boxes + squares).save(final_annotated_path)\n",
    "logging.info(f\"Saved final annotated image {final_annotated_path}\")\n",
    "overlay_generations(generated_faces, img).save(final_save_path)\n",
    "logging.info(f\"Saved final image {final_save_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
