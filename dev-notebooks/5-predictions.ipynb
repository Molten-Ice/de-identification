{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root: No GPU detected, using CPU instead.\n",
      "INFO:root:Saved progress images in c:/Users/James/git/de-identification/dev-notebooks/final\n",
      "INFO:root: Model loaded\n",
      "INFO:root: Model training\n",
      "INFO:root: i: [312/5000] Losses:: Complete:0.3389, contextual:0.1469, perceptual:0.1919 (after x0.1), time: 32.23s\n",
      "INFO:root: i: [624/5000] Losses:: Complete:0.3282, contextual:0.1160, perceptual:0.2123 (after x0.1), time: 71.52s\n",
      "INFO:root: i: [936/5000] Losses:: Complete:0.3141, contextual:0.1044, perceptual:0.2096 (after x0.1), time: 119.55s\n",
      "INFO:root: i: [1248/5000] Losses:: Complete:0.2928, contextual:0.0962, perceptual:0.1965 (after x0.1), time: 163.64s\n",
      "INFO:root: i: [1560/5000] Losses:: Complete:0.3122, contextual:0.0917, perceptual:0.2205 (after x0.1), time: 214.30s\n",
      "INFO:root: i: [1872/5000] Losses:: Complete:0.3046, contextual:0.0882, perceptual:0.2164 (after x0.1), time: 253.87s\n",
      "INFO:root: i: [2184/5000] Losses:: Complete:0.3009, contextual:0.0855, perceptual:0.2154 (after x0.1), time: 290.77s\n",
      "INFO:root: i: [2496/5000] Losses:: Complete:0.2837, contextual:0.0819, perceptual:0.2018 (after x0.1), time: 328.82s\n",
      "INFO:root: i: [2808/5000] Losses:: Complete:0.2771, contextual:0.0790, perceptual:0.1981 (after x0.1), time: 366.81s\n",
      "INFO:root: i: [3120/5000] Losses:: Complete:0.2710, contextual:0.0762, perceptual:0.1948 (after x0.1), time: 408.74s\n",
      "INFO:root: i: [3432/5000] Losses:: Complete:0.2750, contextual:0.0736, perceptual:0.2013 (after x0.1), time: 455.04s\n",
      "INFO:root: i: [3744/5000] Losses:: Complete:0.2554, contextual:0.0710, perceptual:0.1844 (after x0.1), time: 503.08s\n",
      "INFO:root: i: [4056/5000] Losses:: Complete:0.2303, contextual:0.0671, perceptual:0.1631 (after x0.1), time: 539.44s\n",
      "INFO:root: i: [4368/5000] Losses:: Complete:0.2360, contextual:0.0640, perceptual:0.1721 (after x0.1), time: 568.84s\n",
      "INFO:root: i: [4680/5000] Losses:: Complete:0.2404, contextual:0.0618, perceptual:0.1786 (after x0.1), time: 598.28s\n",
      "INFO:root: i: [4992/5000] Losses:: Complete:0.2360, contextual:0.0605, perceptual:0.1755 (after x0.1), time: 632.04s\n",
      "INFO:root: Final Losses:: Complete:0.2358, contextual:0.0605, perceptual:0.1754 (after x0.1), time: 632.84s\n",
      "INFO:root: Training finished\n",
      "INFO:root: Training progress images saved: c:/Users/James/git/de-identification/dev-notebooks/final\\5-training-progress-img-0.jpg\n",
      "INFO:root: Training progress images saved: c:/Users/James/git/de-identification/dev-notebooks/final\\5-training-progress-img-1.jpg\n",
      "INFO:root:Saved final annotated image c:/Users/James/git/de-identification/dev-notebooks/final\\8-annotated_final.jpg\n",
      "INFO:root:Saved final image c:/Users/James/git/de-identification/dev-notebooks/final\\final_image.jpg!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from PIL import Image, ImageDraw\n",
    "from facenet_pytorch import MTCNN\n",
    "import imageio\n",
    "import cv2\n",
    "\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "parser.add_argument('--repo-dir', default=\"c:/Users/James/git/de-identification\", help='Path to github repository')\n",
    "parser.add_argument('--image-path', default=\"c:/Users/James/git/de-identification/downloaded-data/num-faces/train/image_data/16070.jpg\", help='Path to github repository')\n",
    "parser.add_argument('--save-folder', default=\"c:/Users/James/git/de-identification/dev-notebooks/final2\", help='Folder to save results in')\n",
    "parser.add_argument('--border-factor', default=0.2, help='Width of border used for context in infilling GAN generation')\n",
    "parser.add_argument('--progress-images', default=True, help='Save progress images & gif in folder')\n",
    "parser.add_argument('--model-version', default='cropped-10-epochs', help='Version of model to use')\n",
    "parser.add_argument('--lr', default=0.0002, help='learning rate to use in training')\n",
    "parser.add_argument('--lam', default=0.1, help='perceptual loss factor')\n",
    "parser.add_argument('--iterations', default=5000, help='Number of iterations to train for')\n",
    "parser.add_argument('--eval-interval', default=0, help='Number of iterations between evaluation')\n",
    "parser.add_argument('--best-of-n', default=1, help='Number of best predictions to use for final prediction')\n",
    "parser.add_argument('--poisson-blending', default=True, help='Use mask for context')\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "repo_dir = args.repo_dir\n",
    "image_path = args.image_path\n",
    "save_folder = args.save_folder\n",
    "border_factor = args.border_factor\n",
    "progress_images = args.progress_images\n",
    "model_version = args.model_version\n",
    "lr = args.lr\n",
    "lam = args.lam\n",
    "iterations = args.iterations\n",
    "eval_interval = args.eval_interval\n",
    "best_of_n = args.best_of_n\n",
    "poisson_blending = args.poisson_blending\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "if eval_interval == 0:\n",
    "    eval_interval = iterations//16\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cpu\":\n",
    "    logging.warning(\" No GPU detected, using CPU instead.\")\n",
    "\n",
    "mtcnn = MTCNN(keep_all=True, device=device) #face detection model\n",
    "\n",
    "def generate_boxes(img, threshold=0.7):\n",
    "    all_boxes, probs, landmarks = mtcnn.detect(img.copy(), landmarks=True)\n",
    "    if all_boxes is None: return []\n",
    "    all_boxes = [[int(x) for x in box] for box in all_boxes] \n",
    "    #gives box values outside of image, e.g. [-6, 135, 69, 229],\n",
    "    all_boxes = [[max(0, box[0]), max(0, box[1]), box[2], box[3]] for box in all_boxes]\n",
    "    boxes = []\n",
    "    for box, prob in zip(all_boxes, probs):\n",
    "        # width = box[2] - box[0]\n",
    "        # height = box[3] - box[1]\n",
    "        #minimum requirement??\n",
    "        if prob >= threshold:\n",
    "            boxes.append(box)\n",
    "        else:\n",
    "            print(box, prob)\n",
    "    return boxes\n",
    "\n",
    "def draw_boxes(img, boxes, masks=None):\n",
    "    \"\"\"Draws boxes (& masks if wanted) on image)\"\"\"\n",
    "    frame_draw = img.copy()\n",
    "    draw = ImageDraw.Draw(frame_draw)\n",
    "    for box in boxes:\n",
    "        colour = (255, 0, 0) if len(box) == 4 else box[4]\n",
    "        draw.rectangle(box[:4], outline=colour, width=3) # box = (x1, y1, x2, y2)\n",
    "\n",
    "    if masks is not None:\n",
    "        for mask in masks:\n",
    "            draw.rectangle(mask[:4], fill=(255,255,255))\n",
    "    return frame_draw\n",
    "\n",
    "def crop_face(face, x, y):\n",
    "    \"\"\"Turns image into a square by cropping\"\"\"\n",
    "    height, width, _ = face.shape\n",
    "    if height > width:\n",
    "        diff = height - width\n",
    "        top_crop = diff // 2\n",
    "        bottom_crop = diff - top_crop\n",
    "        face = face[top_crop:-bottom_crop, :]\n",
    "        y+=top_crop\n",
    "    elif width > height:\n",
    "        diff = width - height\n",
    "        left_crop = diff // 2\n",
    "        right_crop = diff - left_crop\n",
    "        face = face[:, left_crop:-right_crop]\n",
    "        x+=left_crop\n",
    "\n",
    "    assert face.shape[0] == face.shape[1], \"Face is not square\"\n",
    "    return face, x, y\n",
    "\n",
    "img = Image.open(image_path)\n",
    "boxes = generate_boxes(img)\n",
    "np_img = np.array(img)\n",
    "\n",
    "faces = [] # [[square_face, x, y], ...]\n",
    "\n",
    "squares = []\n",
    "masks = []\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2 = box\n",
    "    face = np_img[y1:y2, x1:x2]\n",
    "    square_face, x, y = crop_face(face, x1, y1)\n",
    "    faces.append([square_face, x, y])\n",
    "    #4 lines below are only for masking visualisation\n",
    "    square_size = square_face.shape[0]\n",
    "    squares.append([x, y, x+square_size, y+square_size, (0, 255, 0)])\n",
    "    square_border = int(square_size * border_factor)\n",
    "    masks.append([x+square_border, y+square_border, x+square_size-square_border, y+square_size-square_border])\n",
    "\n",
    "if progress_images:\n",
    "    img.save(os.path.join(save_folder, \"1-original.jpg\"))\n",
    "    draw_boxes(img, boxes).save(os.path.join(save_folder, \"2-boxes.jpg\"))\n",
    "    draw_boxes(img, boxes + squares).save(os.path.join(save_folder, \"3-boxes_squares.jpg\"))\n",
    "    draw_boxes(img, boxes + squares, masks).save(os.path.join(save_folder, \"4-boxes_squares_masks.jpg\"))\n",
    "    logging.info(f\"Saved progress images in {save_folder}\")\n",
    "\n",
    "######### MODELS #########\n",
    "\n",
    "### Hyperparameters\n",
    "workers = 2\n",
    "image_size = 64 # use 128 but only generates central 64x64\n",
    "border = int(image_size * border_factor)\n",
    "ngpu = 1 # Number of GPUs available. \n",
    "nc = 3 # Number of channels in the training images. \n",
    "nz = 100 # Size of z latent vector (i.e. size of generator input)\n",
    "ngf = 64 # Size of feature maps in generator\n",
    "ndf = 64 # Size of feature maps in discriminator\n",
    "\n",
    "# Generator Code\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*8) x 4 x 4``\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*4) x 8 x 8``\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*2) x 16 x 16``\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf) x 32 x 32``\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. ``(nc) x 64 x 64``\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is ``(nc) x 64 x 64``\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf) x 32 x 32``\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*2) x 16 x 16``\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*4) x 8 x 8``\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*8) x 4 x 4``\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "#load models\n",
    "g_path = os.path.join(repo_dir, f\"pretrained-models/DCGAN-{model_version}-netG.pth\")\n",
    "d_path = os.path.join(repo_dir, f\"pretrained-models/DCGAN-{model_version}-netD.pth\")\n",
    "\n",
    "netG_loaded = Generator(ngpu).to(device)\n",
    "netG_loaded.load_state_dict(torch.load(g_path, map_location=device))\n",
    "netG_loaded.eval()\n",
    "\n",
    "netD_loaded = Discriminator(ngpu).to(device)\n",
    "netD_loaded.load_state_dict(torch.load(d_path, map_location=device))\n",
    "netD_loaded.eval()\n",
    "\n",
    "logging.info(\" Model loaded\")\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "def display_img(transformed_img):\n",
    "    img_grid = vutils.make_grid(transformed_img, padding=2, normalize=True).permute(1, 2, 0)\n",
    "    return Image.fromarray((img_grid.numpy() * 255).astype(np.uint8))\n",
    "\n",
    "transformed_images = []\n",
    "for face in faces:\n",
    "    transformed_images.append(img_transforms(Image.fromarray(face[0])))\n",
    "\n",
    "images = torch.stack(transformed_images, dim = 0)\n",
    "# display_img(transformed_images)\n",
    "\n",
    "mask = torch.ones((images.shape[0], 3, image_size, image_size)).to(device)\n",
    "mask[:, :, border:-border, border:-border] = 0\n",
    "\n",
    "best_results = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "logging.info(\" Model training\")\n",
    "for training_num in range(best_of_n):\n",
    "    if best_of_n != 1:\n",
    "        logging.info(f\" Training loop [{training_num+1}/{best_of_n}]\")\n",
    "    zhats = torch.randn(images.shape[0], nz, 1, 1, device=device).requires_grad_()\n",
    "    results = [netG_loaded(zhats).clone()]\n",
    "    optimizer = optim.Adam([zhats], lr=lr)\n",
    "    t_start = time.time()\n",
    "    for i in range(iterations):\n",
    "        generated = netG_loaded(zhats)\n",
    "        contextual_loss = nn.functional.l1_loss(mask*generated, mask*images) # keep outside obscured region the same\n",
    "\n",
    "        real_label = torch.full((images.shape[0],), 1., dtype=torch.float, device=device)\n",
    "        output = netD_loaded(generated.detach()).view(-1)\n",
    "        perceptual_loss = criterion(output, real_label) #g_loss\n",
    "\n",
    "        complete_loss = contextual_loss + lam*perceptual_loss\n",
    "        optimizer.zero_grad()\n",
    "        complete_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % eval_interval == eval_interval-1:\n",
    "            logging.info(f\" i: [{i+1}/{iterations}] Losses:: Complete:{complete_loss:.4f}, contextual:{contextual_loss:.4f}, perceptual:{lam*perceptual_loss:.4f} (after x0.1), time: {time.time()-t_start:.2f}s\")\n",
    "            results.append(generated.clone())\n",
    "    \n",
    "    logging.info(f\" Final Losses:: Complete:{complete_loss:.4f}, contextual:{contextual_loss:.4f}, perceptual:{lam*perceptual_loss:.4f} (after x0.1), time: {time.time()-t_start:.2f}s\")\n",
    "    if complete_loss.item() < best_loss:\n",
    "        best_loss = complete_loss.item()\n",
    "        best_results = results\n",
    "        best_zhats = zhats\n",
    "\n",
    "logging.info(\" Training finished\")\n",
    "results = best_results\n",
    "zhats = best_zhats.clone()\n",
    "\n",
    "results = torch.stack(results, dim = 0)\n",
    "if progress_images:\n",
    "    training_images = images.expand([results.shape[0]] + list(images.shape))*mask + results*(1-mask)\n",
    "    for face_idx in range(training_images.shape[1]):\n",
    "        training_img_filepath = os.path.join(save_folder, f\"5-training-progress-img-{face_idx}.jpg\")\n",
    "        display_img(training_images[:, face_idx]).save(training_img_filepath)\n",
    "        logging.info(f\" Training progress images saved: {training_img_filepath}\")\n",
    "\n",
    "\n",
    "def overlay_generations(generated_faces, img):\n",
    "    np_img = np.array(img)\n",
    "    for i, (face, x, y), in enumerate(faces):\n",
    "        target_size = face.shape[0]\n",
    "        generated_face = generated_faces[i]\n",
    "        resize_transform = transforms.Resize(target_size, antialias = False)\n",
    "        generated_face = resize_transform(generated_face)\n",
    "        generated_face = display_img(generated_face)\n",
    "\n",
    "        square_border = int(target_size * border_factor)\n",
    "        cropped_img = np.array(generated_face)[square_border:-square_border, square_border:-square_border]\n",
    "        np_img[y+square_border:y+target_size-square_border, x+square_border:x+target_size-square_border] = cropped_img\n",
    "    \n",
    "    return Image.fromarray(np_img)\n",
    "\n",
    "if progress_images:\n",
    "    #Generating gifs\n",
    "    final_images =  []\n",
    "    final_images_annotated = []\n",
    "    for generated_faces in results:\n",
    "        final_image = overlay_generations(generated_faces.cpu(), img)\n",
    "        final_images.append(np.array(final_image))\n",
    "\n",
    "        final_image_annotated = draw_boxes(overlay_generations(generated_faces.cpu(), img), boxes + squares) \n",
    "        final_images_annotated.append(np.array(final_image_annotated))\n",
    "\n",
    "    gif_path = os.path.join(save_folder, f\"6-training_gif.gif\")\n",
    "    imageio.mimsave(gif_path, final_images, duration=0.2)\n",
    "\n",
    "    gif_annotated_path = os.path.join(save_folder, f\"7-training_gif_annotations.gif\")\n",
    "    imageio.mimsave(gif_annotated_path, final_images_annotated, duration=0.5)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_faces = netG_loaded(zhats).cpu()\n",
    "\n",
    "    final_annotated_path = os.path.join(save_folder, f\"8-annotated_final.jpg\")\n",
    "    final_save_path = os.path.join(save_folder, f\"final_image.jpg\")\n",
    "\n",
    "np_img = np.array(img)\n",
    " \n",
    "# Generating predictions\n",
    "for i, (face, x, y), in enumerate(faces):\n",
    "    target_size = face.shape[0]\n",
    "    border_width = int(target_size * border_factor)\n",
    "\n",
    "    generated_face = generated[i]\n",
    "    resize_transform = transforms.Resize(target_size, antialias = False)\n",
    "    generated_face = resize_transform(generated_face)\n",
    "\n",
    "    generated_face = display_img(generated_face)\n",
    "    original_face = Image.fromarray(face)\n",
    "\n",
    "    open_cv_mask = np.ones((target_size, target_size, 3))\n",
    "    open_cv_mask[border_width:-border_width, border_width:-border_width] = 0\n",
    "\n",
    "    original_face_opencv = np.array(original_face)\n",
    "    original_face_opencv = original_face_opencv[:, :, ::-1].copy()  # RGB -> BGR\n",
    "    generated_face_opencv = np.array(generated_face)\n",
    "    generated_face_opencv = generated_face_opencv[:, :, ::-1].copy() \n",
    "\n",
    "    src_mask = np.zeros(original_face_opencv.shape, original_face_opencv.dtype)\n",
    "    square = np.array([[border_width,border_width], [target_size-border_width,border_width], [target_size-border_width,target_size-border_width], [border_width, target_size-border_width]], np.int32)\n",
    "    cv2.fillPoly(src_mask, [square], (255, 255, 255))\n",
    "    center = (target_size//2,target_size//2)\n",
    "\n",
    "    raw_combined = original_face_opencv*open_cv_mask + generated_face_opencv*(1-open_cv_mask)\n",
    "    output = cv2.seamlessClone(generated_face_opencv, original_face_opencv, src_mask, center, cv2.NORMAL_CLONE)\n",
    "    # display(Image.fromarray(raw_combined.astype(np.uint8)[:, :, ::-1]))\n",
    "    # display(Image.fromarray(output[:, :, ::-1]))\n",
    "\n",
    "    if poisson_blending:\n",
    "        generated_face_final = np.array(output[:, :, ::-1])\n",
    "    else:\n",
    "        generated_face_final = raw_combined[:, :, ::-1]\n",
    "\n",
    "    cropped_img = generated_face_final[border_width:-border_width, border_width:-border_width]\n",
    "    np_img[y+border_width:y+target_size-border_width, x+border_width:x+target_size-border_width] = cropped_img\n",
    "\n",
    "final_img = Image.fromarray(np_img)\n",
    "\n",
    "draw_boxes(final_img, boxes + squares).save(final_annotated_path)\n",
    "logging.info(f\"Saved final annotated image {final_annotated_path}\")\n",
    "final_img.save(final_save_path)\n",
    "logging.info(f\"Saved final image {final_save_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
